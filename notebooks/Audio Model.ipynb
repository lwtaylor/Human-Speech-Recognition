{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b37eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bfbe266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionAudioDataset(Dataset):\n",
    "    # audio_dir example: '../data/Crema'\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.transformation = transformation\n",
    "        \n",
    "        wav_file_paths = os.listdir(audio_dir)\n",
    "        \n",
    "        # create data frame out of wav_files\n",
    "        # emotion is listed and can be used by label\n",
    "        # idk what notsure is again\n",
    "        emotions = pd.DataFrame(wav_file_paths, columns=['filename'])\n",
    "        emotions['filename'] = emotions['filename'].str.split('_')\n",
    "        emotions = pd.DataFrame(emotions['filename'].tolist(), columns=['id', 'notsure', 'emotion', 'version'])\n",
    "        emotions['filename'] = wav_file_paths\n",
    "        \n",
    "        # emotions data frame with file path\n",
    "        self.emotions = emotions\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return len of dataframe\n",
    "        return len(self.emotions)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        \n",
    "        # load audio file with torch audio\n",
    "        # i think 2 channels is a stereo audio\n",
    "        # signal -> (num_channels, samples) --> (2, 16000) --> (1, 16000)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        \n",
    "        # resize the array so that they all match in size\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        \n",
    "        # transform the signal with a mel spectogram \n",
    "        # that is passed in \n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "        \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        # get the fold (idk what that is do we have that?)\n",
    "        # index 4 corresponds to the file name\n",
    "        path = os.path.join(self.audio_dir, self.emotions.iloc[index, 4])\n",
    "        \n",
    "        return path \n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        # index 2 of columns corresponds to the emotion label\n",
    "        return self.emotions.iloc[index, 2]\n",
    "    \n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        # if we have a signal with multiple channels \n",
    "        # we will need to mix the signal down from stereo (or whatever)\n",
    "        # and make it mono\n",
    "        \n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        # signal -> Tensor -> (1, num_samples) -> (1, 50,000)\n",
    "        \n",
    "        # if the signal has more samples than the expected number\n",
    "        # of samples, we need to cut it down\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - lenghth_signal \n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50bafd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"main\" section of our py file \n",
    "# if __name__ == \"__main__\":\n",
    "AUDIO_DIR = '../data/Crema'\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# ms = mel_spectogram(signal)\n",
    "# mel_spectogram will be applied to the signal like\n",
    "# this because torchaudio.transforms objects can\n",
    "# be treated like funciton \n",
    "\n",
    "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
    "sample_rate = SAMPLE_RATE, \n",
    "n_fft=1024, \n",
    "hop_length=512,\n",
    "n_mels=64)\n",
    "\n",
    "ead = EmotionAudioDataset(AUDIO_DIR, mel_spectogram, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808f969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
